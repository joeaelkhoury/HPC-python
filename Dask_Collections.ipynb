{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Eb3uPd7Ywj"
      },
      "source": [
        "# Dask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7KvEt07Ywq"
      },
      "source": [
        "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\"\n",
        "     align=\"right\"\n",
        "     width=\"30%\"\n",
        "     alt=\"Dask logo\\\">\n",
        "\n",
        "## Parallelize code with `dask.delayed`\n",
        "\n",
        "In this notebook we parallelize simple for-loop style code with Dask and `dask.delayed`. Often, this is the only function that you will need to convert functions for use with Dask.\n",
        "\n",
        "We will then go on and take a look at Dask DataFrames and how they compute\n",
        "\n",
        "**Related Documentation**\n",
        "\n",
        "* [Delayed documentation](https://docs.dask.org/en/latest/delayed.html)\n",
        "* [Delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU)\n",
        "* [Delayed API](https://docs.dask.org/en/latest/delayed-api.html)\n",
        "* [Delayed examples](https://examples.dask.org/delayed.html)\n",
        "* [Delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_oxmMIv7Yws"
      },
      "source": [
        "As we'll see in the [distributed scheduler notebook](05_distributed.ipynb), Dask has several ways of executing code in parallel. For now we will just use the default implementation to let the tasks run locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsg70Pny7Yws"
      },
      "source": [
        "First let's make some toy functions, `inc` and `add`, that sleep for a while to simulate work. We'll then time running these functions normally.\n",
        "\n",
        "In the next section we'll parallelize this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVgIfKxM7Ywt"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "\n",
        "def inc(x):\n",
        "    sleep(1)\n",
        "    return x + 1\n",
        "\n",
        "def add(x, y):\n",
        "    sleep(1)\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqhlz8wl7Ywu"
      },
      "source": [
        "We time the execution of this normal code using the `%%time` magic, which is a special function of the Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY9JZnyL7Ywv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This takes three seconds to run because we call each\n",
        "# function sequentially, one after the other\n",
        "\n",
        "x = inc(1)\n",
        "y = inc(2)\n",
        "z = add(x, y)\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Yn3cIc7Yww"
      },
      "source": [
        "Those two increment calls *could* be called in parallel, because they are totally independent of one-another.\n",
        "\n",
        "We'll transform the `inc` and `add` functions using the `dask.delayed` function. When we call the delayed version by passing the arguments, exactly as before, the original function isn't actually called yet - which is why the cell execution finishes very quickly.\n",
        "Instead, a *delayed object* is made, which keeps track of the function to call and the arguments to pass to it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt66OPAL7Ywx"
      },
      "source": [
        "### Delay, visualize, compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhcf1SMf7Ywx"
      },
      "outputs": [],
      "source": [
        "from dask import delayed\n",
        "import dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbiH7fmT7Ywy"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This runs immediately, as it does not compute anything.\n",
        "# All it does is build a graph.\n",
        "\n",
        "x = delayed(inc)(1)\n",
        "y = delayed(inc)(2)\n",
        "z = delayed(add)(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX2bG6tG7Ywz"
      },
      "source": [
        "This ran immediately, since nothing has really happened yet.\n",
        "\n",
        "To get the result, call `compute`. Notice that this runs faster than the original code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAknUbCX7Ywz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# This actually runs our computation using a local thread pool\n",
        "\n",
        "z.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "t8EosYgT7Yw0"
      },
      "source": [
        "The `z` object is a lazy `Delayed` object.  This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another.  We can evaluate the result with `.compute()` as above or we can visualize the task graph for this value with `.visualize()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KESzz9PH7Yw0"
      },
      "outputs": [],
      "source": [
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RKNZDEU7Yw0"
      },
      "outputs": [],
      "source": [
        "# Look at the task graph for `z`\n",
        "z.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNTIx9R37Yw0"
      },
      "source": [
        "Notice that this includes the names of the functions from before, and the logical flow of the outputs of the `inc` functions to the inputs of `add`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQse2Lxk7Yw0"
      },
      "source": [
        "### Some questions to consider:\n",
        "\n",
        "-  Why did we go from 3s to 2s?  Why weren't we able to parallelize down to 1s?\n",
        "-  What would have happened if the inc and add functions didn't include the `sleep(1)`?  Would Dask still be able to speed up this code?\n",
        "-  What if we have multiple outputs or also want to get access to x or y?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVtfpcjn7Yw1"
      },
      "outputs": [],
      "source": [
        "def inc(x):\n",
        "    #sleep(1)\n",
        "    return x + 1\n",
        "\n",
        "def add(x, y):\n",
        "    #sleep(1)\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxokzsxG7Yw1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "x = inc(1)\n",
        "y = inc(2)\n",
        "z = add(x, y)\n",
        "x,y,z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2_DBfLM7Yw1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "x = delayed(inc)(1)\n",
        "y = delayed(inc)(2)\n",
        "z = delayed(add)(x, y)\n",
        "dask.compute(x,y,z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7fMUiXx7Yw2"
      },
      "source": [
        "### Exercise: Parallelize a for loop\n",
        "\n",
        "`for` loops are one of the most common things that we want to parallelize.  Use `dask.delayed` on `inc` and `sum` to parallelize the computation below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQMqTs-t7Yw2"
      },
      "outputs": [],
      "source": [
        "data = [1, 2, 3, 4, 5, 6, 7, 8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6LsQ9gq7Yw2"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "\n",
        "def inc(x):\n",
        "    sleep(1)\n",
        "    return x + 1\n",
        "\n",
        "def add(x, y):\n",
        "    sleep(1)\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UcY-GYz7Yw2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Sequential code\n",
        "\n",
        "results = []\n",
        "for x in data:\n",
        "    y = inc(x)\n",
        "    results.append(y)\n",
        "\n",
        "total = sum(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNnw6Fw07Yw2"
      },
      "outputs": [],
      "source": [
        "total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A1dUlfb7Yw2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Your parallel code here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "c0b_u1eU7Yw3"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0BOYyBET7Yw3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results = []\n",
        "\n",
        "for x in data:\n",
        "    y = delayed(inc)(x)\n",
        "    results.append(y)\n",
        "\n",
        "total = delayed(sum)(results)\n",
        "print(\"Before computing:\", total)  # Let's see what total is\n",
        "result = total.compute()\n",
        "print(\"After computing :\", result)  # After it's computed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6xTBoYW7Yw3"
      },
      "source": [
        "How do the graph visualizations compare with the given solution, compared to a version with the `sum` function used directly rather than wrapped with `delayed`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5XDi6AW7Yw3"
      },
      "outputs": [],
      "source": [
        "total.visualize() # sum function wrapped with delayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQkSa5QH7Yw3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results = []\n",
        "\n",
        "for x in data:\n",
        "    y = delayed(inc)(x)\n",
        "    results.append(y)\n",
        "\n",
        "total = sum(results)\n",
        "\n",
        "total.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgRGdrrZ7Yw3"
      },
      "outputs": [],
      "source": [
        "total.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "YGT4BGqA7Yw4"
      },
      "source": [
        "### Parallelizing a for-loop code with control flow\n",
        "\n",
        "Often we want to delay only *some* functions, running a few of them immediately.  This is especially helpful when those functions are fast and help us to determine what other slower functions we should call.  This decision, to delay or not to delay, is usually where we need to be thoughtful when using `dask.delayed`.\n",
        "\n",
        "In the example below we iterate through a list of inputs.  If that input is even then we want to call `inc`.  If the input is odd then we want to call `double`.  This `is_even` decision to call `inc` or `double` has to be made immediately (not lazily) in order for our graph-building Python code to proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd0t4kA_7Yw4"
      },
      "outputs": [],
      "source": [
        "def double(x):\n",
        "    sleep(1)\n",
        "    return 2 * x\n",
        "\n",
        "def is_even(x):\n",
        "    return not x % 2\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_ZCZSi7Yw4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Sequential code\n",
        "\n",
        "results = []\n",
        "for x in data:\n",
        "    if is_even(x):\n",
        "        y = double(x)\n",
        "    else:\n",
        "        y = inc(x)\n",
        "    results.append(y)\n",
        "\n",
        "total = sum(results)\n",
        "print(total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anQcO66N7Yw4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# TODO: parallelize the sequential code above using dask.delayed\n",
        "# You will need to delay some functions, but not all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6SmR_i-L7Yw4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results = []\n",
        "for x in data:\n",
        "    if is_even(x):  # even\n",
        "        y = delayed(double)(x)\n",
        "    else:          # odd\n",
        "        y = delayed(inc)(x)\n",
        "    results.append(y)\n",
        "\n",
        "total = delayed(sum)(results)\n",
        "total.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiyRn6iW7Yw5"
      },
      "outputs": [],
      "source": [
        "total.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diMe3eN87YxF"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results = []\n",
        "for x in data:\n",
        "        if is_even(x):  # even\n",
        "            y = delayed(double)(x)\n",
        "        else:          # odd\n",
        "            y = delayed(inc)(x)\n",
        "        results.append(y)\n",
        "\n",
        "total = sum(results)\n",
        "total.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWcbOmIB7YxF"
      },
      "outputs": [],
      "source": [
        "total.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "59Xg57mr7YxF"
      },
      "source": [
        "### Some questions to consider:\n",
        "\n",
        "-  What would have happened if we had delayed the evaluation of `is_even(x)` in the example above?\n",
        "-  What are your thoughts on delaying `sum`?  This function is both computational but also fast to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "yEn2Z2Bx7YxG"
      },
      "source": [
        "### Parallelizing a Pandas Groupby Reduction\n",
        "\n",
        "In this exercise we read several CSV files and perform a groupby operation in parallel.  We are given sequential code to do this and parallelize it with `dask.delayed`.\n",
        "\n",
        "The computation we will parallelize is to compute the mean departure delay per airport from some historical flight data.  We will do this by using `dask.delayed` together with `pandas`.  In a future section we will do this same exercise with `dask.dataframe`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw4xr2z67YxG"
      },
      "source": [
        "#### Sequential code: Mean Departure Delay Per Airport\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTWAYAPv7YxG"
      },
      "source": [
        "In the previous notebook on pandas, we used concat() and a for-loop to read multiple files into a DataFrame. Here we also perform a groupby operation as well as a sum and count within the for-loop to finally compute the mean manually. This should help visualise the steps that can be parallelised later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXOmTdwv7YxG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spOaiHJq7YxG"
      },
      "outputs": [],
      "source": [
        "filenames = sorted(glob(os.path.join('data', 'nycflights', '*.csv')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn_JDYeS7YxH"
      },
      "outputs": [],
      "source": [
        "filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pakUwjXp7YxH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "sums = []\n",
        "counts = []\n",
        "for fn in filenames:\n",
        "    # Read in file\n",
        "    pdf = pd.read_csv(fn)\n",
        "\n",
        "    # Groupby origin airport\n",
        "    by_origin = pdf.groupby('Origin')\n",
        "\n",
        "    # Sum of all departure delays by origin\n",
        "    total = by_origin.DepDelay.sum()\n",
        "\n",
        "    # Number of flights by origin\n",
        "    count = by_origin.DepDelay.count()\n",
        "\n",
        "    # Save the intermediates\n",
        "    sums.append(total)\n",
        "    counts.append(count)\n",
        "\n",
        "# Combine intermediates to get total mean-delay-per-origin\n",
        "total_delays = sum(sums)\n",
        "n_flights = sum(counts)\n",
        "mean = total_delays / n_flights\n",
        "mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOYrVfxh7YxH"
      },
      "source": [
        "#### Parallel code: Mean Departure Delay Per Airport\n",
        "\n",
        "Use `dask.delayed` to parallelize the code above.  Some extra things you will need to know.\n",
        "\n",
        "1.  Methods and attribute access on delayed objects work automatically, so if you have a delayed object you can perform normal arithmetic, slicing, and method calls on it and it will produce the correct delayed calls.\n",
        "\n",
        "    ```python\n",
        "    x = delayed(np.arange)(10)\n",
        "    y = (x + 1)[::2].sum()  # everything here was delayed\n",
        "    ```\n",
        "2.  Calling the `.compute()` method works well when you have a single output.  When you have multiple outputs you might want to use the `dask.compute` function:\n",
        "\n",
        "    ```python\n",
        "    >>> from dask import compute\n",
        "    >>> x = delayed(np.arange)(10)\n",
        "    >>> y = x ** 2\n",
        "    >>> min_, max_ = compute(y.min(), y.max())\n",
        "    >>> min_, max_\n",
        "    (0, 81)\n",
        "    ```\n",
        "    \n",
        "    This way Dask can share the intermediate values (like `y = x**2`)\n",
        "    \n",
        "So your goal is to parallelize the code above (which has been copied below) using `dask.delayed`.  You may also want to visualize a bit of the computation to see if you're doing it correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BNK1zeC7YxH"
      },
      "outputs": [],
      "source": [
        "from dask import compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvVFtRZA7YxI"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PlLUGAw7YxI"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "JBfI8y5q7YxI"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# This is just one possible solution, there are\n",
        "# several ways to do this using `delayed`\n",
        "\n",
        "sums = []\n",
        "counts = []\n",
        "for fn in filenames:\n",
        "    # Read in file\n",
        "    ddf = delayed(pd.read_csv)(fn)\n",
        "\n",
        "    # Groupby origin airport\n",
        "    by_origin = ddf.groupby('Origin')\n",
        "\n",
        "    # Sum of all departure delays by origin\n",
        "    total = by_origin.DepDelay.sum()\n",
        "\n",
        "    # Number of flights by origin\n",
        "    count = by_origin.DepDelay.count()\n",
        "\n",
        "    # Save the intermediates\n",
        "    sums.append(total)\n",
        "    counts.append(count)\n",
        "\n",
        "# Compute the intermediates\n",
        "#sums, counts = compute(sums, counts)\n",
        "\n",
        "# Combine intermediates to get total mean-delay-per-origin\n",
        "total_delays = sum(sums)\n",
        "n_flights = sum(counts)\n",
        "#total_delays = delayed(sum)(sums)\n",
        "#n_flights = delayed(sum)(counts)\n",
        "#total_delays, n_flights = compute(total_delays, n_flights)\n",
        "mean = total_delays / n_flights\n",
        "mean.compute()\n",
        "#mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "iNXp4tsf7YxJ"
      },
      "outputs": [],
      "source": [
        "mean.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "1PWovXOO7YxJ"
      },
      "source": [
        "### Some questions to consider:\n",
        "\n",
        "- How much speedup did you get? Is this how much speedup you'd expect?\n",
        "- Experiment with where to call `compute`. What happens when you call it on `sums` and `counts`? What happens if you wait and call it on `mean`?\n",
        "- Experiment with delaying the call to `sum`. What does the graph look like if `sum` is delayed? What does the graph look like if it isn't?\n",
        "- Can you think of any reason why you'd want to do the reduction one way over the other?\n",
        "\n",
        "### Learn More\n",
        "\n",
        "Visit the [Delayed documentation](https://docs.dask.org/en/latest/delayed.html). In particular, this [delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU) will reinforce the concepts you learned here and the [delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html) document collects advice on using `dask.delayed` well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPzOfrkR7YxJ"
      },
      "source": [
        "## Dask DataFrames\n",
        "In this section we use `dask.dataframe` to automatically build similar computations as we just performed, for the common case of tabular computations. Dask dataframes look and feel like Pandas dataframes but they run on the same infrastructure that powers dask.delayed.\n",
        "\n",
        "In this chapter we use the same airline data as before, but now rather than write for-loops we let dask.dataframe construct our computations for us. The `dask.dataframe.read_csv` function can take a globstring like \"data/nycflights/*.csv\" and build parallel computations on all of our data at once.\n",
        "\n",
        "Pandas is great for tabular datasets that fit in memory. Dask becomes useful when the dataset you want to analyse is larger than your machine's RAM. Our dataset does not bring the avaliable memory to its limits, but we can still demonstrate how you go about making use of `dask.dataframe`.\n",
        "\n",
        "The `dask.dataframe` module implements a blocked parallel `DataFrame` object that mimics a large subset of the Pandas `DataFrame` API. One Dask `DataFrame` is comprised of many in-memory pandas `DataFrames` separated along the index. One operation on a Dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.\n",
        "\n",
        "For the most part, a Dask DataFrame feels like a pandas DataFrame.\n",
        "So far, the biggest difference we've seen is that Dask operations are lazy; they build up a task graph instead of executing immediately.\n",
        "This lets Dask do operations in parallel and out of core.\n",
        "\n",
        "A Dask DataFrame is composed of many pandas DataFrames. For `dask.dataframe` the chunking happens along the index.\n",
        "\n",
        "<img src=\"http://docs.dask.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">\n",
        "\n",
        "We call each chunk a *partition*, and the upper / lower bounds are *divisions*.\n",
        "Dask *can* store information about the divisions. For now, partitions come up when you write custom functions to apply to Dask DataFrames\n",
        "**Main Take-aways**\n",
        "\n",
        "1.  Dask DataFrame should be familiar to Pandas users\n",
        "2.  The partitioning of dataframes is important for efficient execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee4NHo_u7YxJ"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wjXlsZX7YxK"
      },
      "source": [
        "#### Single Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucOmnKV7YxK"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "import dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjn_E93P7YxK"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_csv(\"./data/nycflights/1999.csv\") # With Dask, you can also enter a url instead of a directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "U7O1VBfp7YxK"
      },
      "outputs": [],
      "source": [
        "ddf.head(3) # This works just as in pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmDLiwVu7YxK"
      },
      "outputs": [],
      "source": [
        "ddf.tail(3) # And so does this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKYMci3C7YxL"
      },
      "source": [
        "Watch out for wrongly interpreted data types in Dask! Dask is lazy in every way and you possibly need to manually enter some data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_b61jQV7YxL"
      },
      "outputs": [],
      "source": [
        "ddf.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvBACIqD7YxL"
      },
      "source": [
        "#### Mulitple Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb5cvc4H7YxL"
      },
      "source": [
        "Dask can intelligently read multiple files into one dataframe with a glob (asterisk):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4NlZCbYY7YxL"
      },
      "outputs": [],
      "source": [
        "filepath = glob(\"./data/nycflights/*.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HKY_WYe7YxM"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu8Q7Hgd7YxM"
      },
      "outputs": [],
      "source": [
        "ddf.head(3) # this works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-2thBkK7YxM"
      },
      "outputs": [],
      "source": [
        "#ddf.tail(3) # This fails. Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5Ouuu3T7YxM"
      },
      "outputs": [],
      "source": [
        "ddf.dtypes # Let's check the datatypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFCkz8DJ7YxM"
      },
      "source": [
        "Unlike `pandas.read_csv` which reads in the entire file before inferring datatypes, `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\n",
        "\n",
        "In this case, the datatypes inferred in the sample are incorrect. The first `n` rows have no value for `CRSElapsedTime` (which pandas infers as a `float`), and later on turn out to be strings (`object` dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options:\n",
        "\n",
        "- Specify dtypes directly using the `dtype` keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant.\n",
        "- Increase the size of the `sample` keyword (in bytes)\n",
        "- Use `assume_missing` to make `dask` assume that columns inferred to be `int` (which don't allow missing values) are actually floats (which do allow missing values). In our particular case this doesn't apply.\n",
        "\n",
        "In our case we'll use the first option and directly specify the `dtypes` of (just) the offending columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svqupgnb7YxN"
      },
      "outputs": [],
      "source": [
        "ddf = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
        "                 parse_dates={'Date': [0, 1, 2]}, # Here we parse the year, month and day into date\n",
        "                 dtype={'TailNum': str,\n",
        "                        'CRSElapsedTime': float,\n",
        "                        'Cancelled': bool})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJIsM3MN7YxN"
      },
      "source": [
        "Notice that the respresentation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugdcKgTu7YxN"
      },
      "outputs": [],
      "source": [
        "ddf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XtoFodV7YxN"
      },
      "outputs": [],
      "source": [
        "ddf.tail(3) # Now it works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKq61Xxo7YxO"
      },
      "source": [
        "Let's also read the holidays data which will use in the exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7IFPD4_7YxO"
      },
      "outputs": [],
      "source": [
        "holidays = dd.read_parquet(os.path.join('data', \"holidays\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wH48JsrZ7YxO"
      },
      "outputs": [],
      "source": [
        "holidays.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "h0v8RX2k7YxO"
      },
      "source": [
        "### Computations with `dask.dataframe`\n",
        "\n",
        "We compute the maximum of the `DepDelay` column. With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums\n",
        "\n",
        "```python\n",
        "maxes = []\n",
        "for fn in filenames:\n",
        "    pdf = pd.read_csv(fn)\n",
        "    maxes.append(df[\"DepDelay\"].max())\n",
        "    \n",
        "final_max = max(maxes)\n",
        "```\n",
        "\n",
        "We could wrap that `pd.read_csv` with `dask.delayed` so that it runs in parallel. Regardless, we're still having to think about loops, intermediate results (one per file) and the final reduction (`max` of the intermediate maxes). This is just noise around the real task, which pandas solves with\n",
        "\n",
        "```python\n",
        "pdf = pd.read_csv(filename, dtype=dtype)\n",
        "pdf[\"DepDelay\"].max()\n",
        "```\n",
        "\n",
        "`dask.dataframe` lets us write pandas-like code, that operates on larger than memory datasets in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd5MuE1U7YxO"
      },
      "source": [
        "Let's compare the performance of data loading and computation to pandas.\n",
        "First we are going to time this in Dask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lLOuwEn7YxP"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "ddf = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
        "                 parse_dates={'Date': [0, 1, 2]}, # Here we parse the year, month and day into date\n",
        "                 dtype={'TailNum': str,\n",
        "                        'CRSElapsedTime': float,\n",
        "                        'Cancelled': bool})\n",
        "ddf[\"DepDelay\"].max().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRThZxBe7YxP"
      },
      "source": [
        "Let's compare this to pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dbXoha67YxP"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "filepath = glob(\"./data/nycflights/*.csv\")\n",
        "pdf = pd.concat(pd.read_csv(f) for f in filepath)\n",
        "pdf[\"DepDelay\"].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMixeY2E7YxP"
      },
      "outputs": [],
      "source": [
        "pdf = pd.concat(pd.read_csv(f) for f in filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIaSR9Nm7YxW"
      },
      "outputs": [],
      "source": [
        "%time pdf[\"DepDelay\"].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a0szOFl7YxW"
      },
      "source": [
        "This writes the delayed computation for us and then runs it.  \n",
        "\n",
        "Some things to note:\n",
        "\n",
        "1.  As with `dask.delayed`, we need to call `.compute()` when we're done.  Up until this point everything is lazy.\n",
        "2.  Dask will delete intermediate results (like the full pandas dataframe for each file) as soon as possible.\n",
        "    -  This lets us handle datasets that are larger than memory\n",
        "    -  This means that repeated computations will have to load all of the data in each time (run the code above again, is it faster or slower than you would expect?)\n",
        "    \n",
        "As with `Delayed` objects, you can view the underlying task graph using the `.visualize` method (notice the parallelism):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYS8R1xX7YxX"
      },
      "outputs": [],
      "source": [
        "ddf.DepDelay.max().visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5ril24g7YxX"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "In this section we do a few `dask.dataframe` computations. If you are comfortable with pandas then these should be familiar. You will have to think about when to call `compute`.\n",
        "\n",
        "#### 1.) How many rows are in our dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp14989V7YxX"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqKmIpX_7YxX"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "DXsWV9Fu7YxX"
      },
      "outputs": [],
      "source": [
        "len(ddf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2wZdTXJ7YxY"
      },
      "source": [
        "#### 2.) In total, how many non-canceled flights were taken?\n",
        "\n",
        "With pandas, you would use [boolean indexing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PU9qoPH7YxY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zjDXUrB7YxY"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "liGvw3RE7YxY"
      },
      "outputs": [],
      "source": [
        "len(ddf[ddf.Cancelled==False])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_0D63Ja7YxY"
      },
      "source": [
        "#### 3.) What was the average departure delay from each airport?\n",
        "*Hint*: use [`ddf.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html).\n",
        "\n",
        "Note, this is the same computation you did in the previous notebook (is this approach faster or slower?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ce17nju7YxZ"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjM9q-Ft7YxZ"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "Trz0Gz3C7YxZ"
      },
      "outputs": [],
      "source": [
        "%time ddf[\"DepDelay\"].groupby(ddf[\"Origin\"]).mean().compute()\n",
        "# Alternative solution:\n",
        "# ddf.groupby(\"Origin\").DepDelay.mean().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIOTNAJs7YxZ"
      },
      "source": [
        "#### 4.) In total, how many non-cancelled flights were taken from each airport?\n",
        "\n",
        "*Hint*: use [`ddf.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYyZ5-n07YxZ"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NghLGm4U7Yxa"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "kUJgdSKK7Yxa"
      },
      "outputs": [],
      "source": [
        "ddf[~ddf.Cancelled].groupby(\"Origin\").Origin.count().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPX-hnp7Yxa"
      },
      "source": [
        "#### 5.) What day of the week has the worst average departure delay?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgjidP4N7Yxa"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7-z817C7Yxa"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "E8oFE8kG7Yxb"
      },
      "outputs": [],
      "source": [
        "ddf[\"DepDelay\"].groupby(ddf[\"DayOfWeek\"]).mean().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-sew5SU7Yxb"
      },
      "source": [
        "#### 6.) What holiday has the worst average departure delay?\n",
        "\n",
        "*Hint*: use [`ddf.merge`](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) to bring holiday information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmaaPh6M7Yxb"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNKnkJ6k7Yxb"
      },
      "source": [
        "Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "5Um47UJP7Yxc"
      },
      "outputs": [],
      "source": [
        "%time ddf.merge(holidays, on=[\"Date\"], how=\"left\").groupby(\"holiday\").DepDelay.mean().compute()\n",
        "\n",
        "# Alternative Solution:\n",
        "#%%time\n",
        "#ddf_merged = ddf.merge(holidays, on=[\"Date\"], how=\"left\")\n",
        "#ddf_merged[\"DepDelay\"].groupby(ddf_merged[\"holiday\"]).mean().compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "eVDguPjf7Yxc"
      },
      "source": [
        "### Sharing Intermediate Results\n",
        "\n",
        "When computing all of the above, we sometimes did the same operation more than once. For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n",
        "\n",
        "For example, lets compute the mean and standard deviation for departure delay of all non-canceled flights. Since dask operations are lazy, those values aren't the final results yet. They're just the recipe required to get the result.\n",
        "\n",
        "If we compute them with two calls to compute, there is no sharing of intermediate computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t81MXebK7Yxc"
      },
      "outputs": [],
      "source": [
        "non_cancelled = ddf[~ddf.Cancelled]\n",
        "mean_delay = non_cancelled.DepDelay.mean()\n",
        "std_delay = non_cancelled.DepDelay.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PVgqKQf7Yxc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "mean_delay_res = mean_delay.compute()\n",
        "std_delay_res = std_delay.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Jj6hU77Yxc"
      },
      "source": [
        "But let's try by passing both to a single `compute` call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puE8etS-7Yxc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "mean_delay_res, std_delay_res = compute(mean_delay, std_delay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhSkGn1C7Yxd"
      },
      "source": [
        "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n",
        "\n",
        "- the calls to `read_csv`\n",
        "- the filter (`df[~df.Cancelled]`)\n",
        "- some of the necessary reductions (`sum`, `count`)\n",
        "\n",
        "To see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function (we might want to use `filename='graph.pdf'` to save the graph to disk so that we can zoom in more easily):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If_n5wUx7Yxd"
      },
      "outputs": [],
      "source": [
        "dask.visualize(mean_delay, std_delay, filename=\"graph.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "MipLGYpi7Yxd"
      },
      "source": [
        "### Conversion to a timestamp\n",
        "\n",
        "This dataset stores timestamps as `HHMM`, which are read in as integers in `read_csv`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePV5MUw87Yxd"
      },
      "outputs": [],
      "source": [
        "crs_dep_time = pdf[\"CRSDepTime\"].head(10)\n",
        "crs_dep_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCW58pbU7Yxd"
      },
      "source": [
        "To convert these to timestamps of scheduled departure time, we need to convert these integers into `pd.Timedelta` objects, and then combine them with the `Date` column.\n",
        "\n",
        "In the pandas notebook, we rounded and converted the data to integers, then to strings and then to the date_time datatype. However, we could also do this using the `pd.to_timedelta` function, and a bit of arithmetic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3njjORc-7Yxe"
      },
      "outputs": [],
      "source": [
        "pdf.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKEILGL67Yxe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Rename column \"DayofMonth\"\n",
        "pdf.rename(columns = {\"DayofMonth\":\"Day\"}, inplace=True)\n",
        "\n",
        "# Parse to Date:\n",
        "pdf[\"Date\"] = pd.to_datetime(pdf[[\"Year\",\"Month\",\"Day\"]])\n",
        "\n",
        "# Get the first 10 dates to complement our `crs_dep_time`\n",
        "date = pdf.Date.head(10)\n",
        "date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvNUElcp7Yxe"
      },
      "outputs": [],
      "source": [
        "# Get hours as an integer, convert to a timedelta\n",
        "hours = crs_dep_time // 100\n",
        "hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
        "\n",
        "# Get minutes as an integer, convert to a timedelta\n",
        "minutes = crs_dep_time % 100\n",
        "minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
        "\n",
        "# Apply the timedeltas to offset the dates by the departure time\n",
        "departure_timestamp = date + hours_timedelta + minutes_timedelta\n",
        "departure_timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjS0UHvT7Yxe"
      },
      "source": [
        "#### Custom code and Dask Dataframe\n",
        "\n",
        "We could swap out `pd.to_timedelta` for `dd.to_timedelta` and do the same operations on the entire dask DataFrame. But let's say that Dask hadn't implemented a `dd.to_timedelta` that works on Dask DataFrames. What would you do then?\n",
        "\n",
        "`dask.dataframe` provides a few methods to make applying custom functions to Dask DataFrames easier:\n",
        "\n",
        "- [`map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)\n",
        "- [`map_overlap`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_overlap)\n",
        "- [`reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)\n",
        "\n",
        "Here we'll just be discussing `map_partitions`, which we can use to implement `to_timedelta` on our own:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbtyZ1_U7Yxf"
      },
      "source": [
        "The basic idea is to apply a function that operates on a DataFrame to each partition.\n",
        "In this case, we'll apply `pd.to_timedelta`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y66-9YPK7Yxf"
      },
      "outputs": [],
      "source": [
        "hours = ddf.CRSDepTime // 100\n",
        "# hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
        "hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')\n",
        "\n",
        "minutes = ddf.CRSDepTime % 100\n",
        "# minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
        "minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')\n",
        "\n",
        "departure_timestamp = ddf.Date + hours_timedelta + minutes_timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uagIcqPn7Yxf"
      },
      "outputs": [],
      "source": [
        "departure_timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQAnnRWf7Yxf"
      },
      "outputs": [],
      "source": [
        "departure_timestamp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEIwKOUM7Yxg"
      },
      "source": [
        "#### Rewrite above to use a single call to `map_partitions`\n",
        "\n",
        "This will be slightly more efficient than two separate calls, as it reduces the number of tasks in the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "A0I8VXNJ7Yxg"
      },
      "outputs": [],
      "source": [
        "def compute_departure_timestamp(ddf):\n",
        "    hours = ddf.CRSDepTime // 100\n",
        "    hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
        "\n",
        "    minutes = ddf.CRSDepTime % 100\n",
        "    minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
        "\n",
        "    return ddf.Date + hours_timedelta + minutes_timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YHm_eVx7Yxg"
      },
      "outputs": [],
      "source": [
        "departure_timestamp = ddf.map_partitions(compute_departure_timestamp)\n",
        "departure_timestamp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MuFdo0U7Yxg"
      },
      "source": [
        "### Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iSU_8qr7Yxg"
      },
      "source": [
        "Dask.dataframe only covers a small but well-used portion of the Pandas API.\n",
        "This limitation is for two reasons:\n",
        "\n",
        "1.  The Pandas API is *huge*\n",
        "2.  Some operations are genuinely hard to do in parallel (e.g. sort)\n",
        "\n",
        "Additionally, some important operations like ``set_index`` work, but are slower\n",
        "than in Pandas because they include substantial shuffling of data, and may write out to disk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkxTbaLO7Yxg"
      },
      "source": [
        "### Learn More\n",
        "\n",
        "\n",
        "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
        "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
        "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
        "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
        "* [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plqpdZRu7Yxh"
      },
      "source": [
        "## Dask Array\n",
        "\n",
        "Parallel, larger-than-memory, n-dimensional array using blocked algorithms.\n",
        "\n",
        "* Parallel: Uses all of the cores on your computer\n",
        "* Larger-than-memory: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
        "* Blocked Algorithms: Perform large computations by performing many smaller computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo8BY34W7Yxh"
      },
      "outputs": [],
      "source": [
        "import dask.array as da\n",
        "import numpy as np\n",
        "import dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikVL-b437Yxh"
      },
      "outputs": [],
      "source": [
        "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3I2HkpY7Yxh"
      },
      "outputs": [],
      "source": [
        "y = x + x.T\n",
        "z = y[::2, 5000:].mean(axis=1)\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XRDiNFu7Yxi"
      },
      "outputs": [],
      "source": [
        "z.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVxjDzRF7Yxi"
      },
      "source": [
        "Let's compare Numpy to Dask. First we time the numpy version of a computation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1FlAHc57Yxi"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "xn = np.random.normal(10, 0.1, size=(30_000, 30_000))\n",
        "yn = xn.mean(axis=0)\n",
        "yn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbCcjA1t7Yxi"
      },
      "outputs": [],
      "source": [
        "del xn, yn # Let's release some memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBXGH-5V7Yxj"
      },
      "source": [
        "And now the Dask version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itlp2MaA7Yxj"
      },
      "outputs": [],
      "source": [
        "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
        "xd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLDP-P_O7Yxj"
      },
      "outputs": [],
      "source": [
        "xd.nbytes / 1e9  # Gigabytes of the input processed lazily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsTD-ZZi7Yxj"
      },
      "outputs": [],
      "source": [
        "yd = xd.mean(axis=0)\n",
        "yd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOYGZ-WO7Yxj"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
        "yd = xd.mean(axis=0)\n",
        "yd.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTysL47m7Yxk"
      },
      "source": [
        "**Questions to think about:**\n",
        "\n",
        "* What happens if the Dask chunks=(10000,10000)?\n",
        "* What happens if the Dask chunks=(30,30)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ67JTba7Yxk"
      },
      "source": [
        "### Exercise:\n",
        "\n",
        "For Dask arrays, compute the mean along axis=1 of the sum of the x array and its transpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DnBq1pQ7Yxk"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smu_P6P87Yxk"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "uB0ZsLBZ7Yxk"
      },
      "outputs": [],
      "source": [
        "x_sum = xd + xd.T\n",
        "res = x_sum.mean(axis=1)\n",
        "res.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l27kQrOo7Yxk"
      },
      "outputs": [],
      "source": [
        "del xd, yd # Another memory release"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "1NgAYWmk7Yxl"
      },
      "source": [
        "### Persist Data in Memory\n",
        "\n",
        "If you have the available RAM for your dataset then you can persist data in memory.\n",
        "This allows future computations to be much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mE1tptiI7Yxl"
      },
      "outputs": [],
      "source": [
        "y = y.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fm29uxh7Yxl"
      },
      "outputs": [],
      "source": [
        "%time y[0, 0].compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw0kN2S17Yxl"
      },
      "outputs": [],
      "source": [
        "%time y.sum().compute()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}